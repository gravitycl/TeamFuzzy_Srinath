{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation and Metadata Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Creation Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n",
      "Loaded 30866 valid images\n",
      "Creating datasets...\n",
      "Training batches: 771\n",
      "Validation batches: 192\n",
      "Building model...\n",
      "Training...\n",
      "Epoch 1/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 363ms/step - accuracy: 0.0098 - loss: 5.4207 - val_accuracy: 0.0166 - val_loss: 5.1137 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 363ms/step - accuracy: 0.0230 - loss: 5.1304 - val_accuracy: 0.0418 - val_loss: 4.9281 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 383ms/step - accuracy: 0.0366 - loss: 4.9599 - val_accuracy: 0.0566 - val_loss: 4.8197 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 369ms/step - accuracy: 0.0484 - loss: 4.8274 - val_accuracy: 0.0692 - val_loss: 4.6786 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 380ms/step - accuracy: 0.0562 - loss: 4.7231 - val_accuracy: 0.0711 - val_loss: 4.6084 - learning_rate: 0.0010\n",
      "Epoch 6/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 366ms/step - accuracy: 0.0637 - loss: 4.6452 - val_accuracy: 0.0789 - val_loss: 4.5228 - learning_rate: 0.0010\n",
      "Epoch 7/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 365ms/step - accuracy: 0.0745 - loss: 4.5604 - val_accuracy: 0.0894 - val_loss: 4.4747 - learning_rate: 0.0010\n",
      "Epoch 8/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 365ms/step - accuracy: 0.0771 - loss: 4.5054 - val_accuracy: 0.1019 - val_loss: 4.4227 - learning_rate: 0.0010\n",
      "Epoch 9/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 369ms/step - accuracy: 0.0849 - loss: 4.4564 - val_accuracy: 0.0946 - val_loss: 4.4130 - learning_rate: 0.0010\n",
      "Epoch 10/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 370ms/step - accuracy: 0.0917 - loss: 4.4087 - val_accuracy: 0.1133 - val_loss: 4.2761 - learning_rate: 0.0010\n",
      "Epoch 11/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 372ms/step - accuracy: 0.0953 - loss: 4.3456 - val_accuracy: 0.1160 - val_loss: 4.2533 - learning_rate: 0.0010\n",
      "Epoch 12/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 383ms/step - accuracy: 0.1024 - loss: 4.3060 - val_accuracy: 0.1115 - val_loss: 4.2271 - learning_rate: 0.0010\n",
      "Epoch 13/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 381ms/step - accuracy: 0.1042 - loss: 4.2769 - val_accuracy: 0.1074 - val_loss: 4.1879 - learning_rate: 0.0010\n",
      "Epoch 14/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 367ms/step - accuracy: 0.1028 - loss: 4.2497 - val_accuracy: 0.1315 - val_loss: 4.1495 - learning_rate: 0.0010\n",
      "Epoch 15/15\n",
      "\u001b[1m771/771\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 369ms/step - accuracy: 0.1128 - loss: 4.2093 - val_accuracy: 0.1214 - val_loss: 4.1104 - learning_rate: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import json\n",
    "\n",
    "def parse_leafsnap_metadata(metadata_path, dataset_root):\n",
    "    \"\"\"Parse metadata with proper path validation\"\"\"\n",
    "    df = pd.read_csv(metadata_path, sep='\\t')\n",
    "    \n",
    "    valid_records = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Check both image paths\n",
    "        img_path = os.path.join(dataset_root, row['image_path'])\n",
    "        seg_path = os.path.join(dataset_root, row['segmented_path'])\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            valid_records.append({'path': img_path, 'species': row['species']})\n",
    "        elif os.path.exists(seg_path):\n",
    "            valid_records.append({'path': seg_path, 'species': row['species']})\n",
    "    \n",
    "    return pd.DataFrame(valid_records)\n",
    "\n",
    "def load_and_preprocess_image(path, label):\n",
    "    \"\"\"TensorFlow-native image processing without Python conditionals\"\"\"\n",
    "    try:\n",
    "        # Read and decode image\n",
    "        img = tf.io.read_file(path)\n",
    "        \n",
    "        # Use TensorFlow string ops to check extension\n",
    "        is_jpeg = tf.strings.regex_full_match(path, \".*\\.jpe?g\")\n",
    "        image = tf.cond(\n",
    "            is_jpeg,\n",
    "            lambda: tf.image.decode_jpeg(img, channels=3),\n",
    "            lambda: tf.image.decode_png(img, channels=3)\n",
    "        )\n",
    "        \n",
    "        # Convert and preprocess\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        image = tf.image.resize_with_pad(image, 224, 224)\n",
    "        image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "        \n",
    "        return image, label\n",
    "    except Exception as e:\n",
    "        # You might log the exception here for debugging purposes\n",
    "        return tf.zeros([224, 224, 3], tf.float32), -1\n",
    "\n",
    "def create_dataset(df, batch_size=32, shuffle=True):\n",
    "    \"\"\"Create dataset with known cardinality\"\"\"\n",
    "    # Create label mapping\n",
    "    species = sorted(df['species'].unique())\n",
    "    label_map = {s: i for i, s in enumerate(species)}\n",
    "    df['label'] = df['species'].map(label_map).astype(np.int32)\n",
    "    \n",
    "    # 1. EXPLICITLY FILTER INVALID ENTRIES FIRST\n",
    "    df = df[df['label'].notna()]\n",
    "    \n",
    "    # 2. CALCULATE DATASET LENGTH BEFORE CREATION\n",
    "    num_samples = len(df)\n",
    "    \n",
    "    # Create dataset from numpy arrays\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df['path'].values, df['label'].values))\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(num_samples)\n",
    "    \n",
    "    # 3. USE KNOWN CARDINALITY\n",
    "    ds = ds.apply(tf.data.experimental.assert_cardinality(num_samples))\n",
    "    \n",
    "    ds = ds.map(load_and_preprocess_image, \n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # 4. BATCH WITH DROP_REMAINDER TO MAINTAIN KNOWN SIZE\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    # Prefetch for performance improvement\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds, num_samples // batch_size\n",
    "\n",
    "def build_model(num_classes):\n",
    "    \"\"\"Create MobileNetV2 model with proper initialization\"\"\"\n",
    "    base = MobileNetV2(input_shape=(224, 224, 3), \n",
    "                       include_top=False, \n",
    "                       weights='imagenet')\n",
    "    base.trainable = False\n",
    "    \n",
    "    inputs = Input((224, 224, 3))\n",
    "    x = base(inputs)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    dataset_root = '/Users/drago/plant_detector/leafsnap-dataset'\n",
    "    metadata_path = f\"{dataset_root}/leafsnap-dataset-images.txt\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Load data\n",
    "        print(\"Loading metadata...\")\n",
    "        df = parse_leafsnap_metadata(metadata_path, dataset_root)\n",
    "        print(f\"Loaded {len(df)} valid images\")\n",
    "        \n",
    "        # 2. Split data\n",
    "        train_df, val_df = train_test_split(\n",
    "            df, test_size=0.2, stratify=df['species'], random_state=42\n",
    "        )\n",
    "        \n",
    "        # 3. Create datasets\n",
    "        print(\"Creating datasets...\")\n",
    "        train_ds, train_steps = create_dataset(train_df, shuffle=True)\n",
    "        val_ds, val_steps = create_dataset(val_df, shuffle=False)\n",
    "        \n",
    "        # Verify dataset sizes\n",
    "        print(f\"Training batches: {train_steps}\")\n",
    "        print(f\"Validation batches: {val_steps}\")\n",
    "        \n",
    "        # 4. Build and train model\n",
    "        print(\"Building model...\")\n",
    "        model = build_model(len(df['species'].unique()))\n",
    "        \n",
    "        print(\"Training...\")\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=15,\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=3, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(factor=0.1, patience=2)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 5. Save model (you can also save in SavedModel format if preferred)\n",
    "        model.save(\"plant_model.h5\")\n",
    "        print(\"Training completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
